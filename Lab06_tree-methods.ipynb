{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marant482/MLclass/blob/main/Lab06_tree-methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Understanding Splitting Criteria in CART for Regression**\n",
        "---------------------\n",
        "\n",
        "In this assignment, you will explore three common formulations of the splitting criterion used in **CART (Classification and Regression Trees)** for **regression problems**:\n",
        "\n",
        "1. **Local RSS Minimization**  \n",
        "2. **RSS Gain Maximization**  \n",
        "3. **Total RSS Minimization**\n",
        "\n",
        "You will investigate whether any of these criteria are equivalent, and you will design an experiment to determine which criterion is actually employed in a standard implementation such as **scikit-learn’s DecisionTreeRegressor**.\n",
        "\n",
        "\n",
        "\n",
        "## **The Problem**\n",
        "\n",
        "Many treatments of CART for regression describe the split selection process in different ways. Below are three frequently cited formulations. Suppose we have a dataset with features $X$ and target $y$, and we seek to choose a feature $X_j$ and a threshold $t$ to split the data into two regions $R_1(X_j, t)$ and $R_2(X_j, t)$. Denote by $\\bar{y}_{R_m}$ the mean of targets within region $R_m$.\n",
        "\n",
        "1. **Local RSS Minimization**  \n",
        "   We select the feature and threshold that minimize the **sum of squared errors** in the two resulting child nodes:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.\n",
        "   $$\n",
        "\n",
        "2. **RSS Gain Maximization**  \n",
        "\n",
        "   It is also a local method, looking only at a parent and two child nodes.\n",
        "\n",
        "   We select the feature and threshold that maximize the **reduction** in RSS, computed by subtracting the RSS of the two child nodes from the RSS in the parent node:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\max_{X_j, t} \\Bigl\\{\n",
        "   \\underbrace{\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2}_{\\text{Parent RSS}}\n",
        "   \\;-\\;\n",
        "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
        "   \\Bigr\\}.\n",
        "   $$\n",
        "\n",
        "3. **Total RSS Minimization**  \n",
        "   For a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with features $X$ and target $y$, let $T$ be the current tree.\n",
        "\n",
        "   For any split on feature $X_j$ at threshold $t$, define $T(X_j, t)$ as the new tree obtained by splitting one leaf of $T$ into two leaves $R_1(X_j, t)$ and $R_2(X_j, t)$.\n",
        "   \n",
        "   Let $\\mathrm{Leaves}(T(X_j, t))$ be the set of all leaf indices in this new tree. For each leaf $m \\in \\mathrm{Leaves}(T(X_j, t))$, define:\n",
        "   $$\n",
        "   R_m = \\{\\, i \\,\\mid\\, x_i \\text{ ends in leaf } m\\}.\n",
        "   $$\n",
        "\n",
        "   $R_m$ set collects all data indices $i$ whose feature vector $x_i$ is classified into the leaf node $m$ when passed through the tree $T(X_j,t)$. In other words, each leaf node $m$ in $T(X_j, t)$ corresponds to a unique path of splits, and any data point $x_i$ that follows that path is assigned to the leaf $m$; hence, it belongs to $R_m$.\n",
        "\n",
        "   $R_m$ sets for all leafs $m \\in \\mathrm{Leaves}(T(X_j, t))$ define a partition of all indices.\n",
        "\n",
        "   Then the objective of **minimizing total Residual Sum of Squares (total RSS)** is stated as:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\min_{(X_j, t)} \\sum_{m \\in \\mathrm{Leaves}(T(X_j, t))}\n",
        "   \\sum_{i \\in R_m} \\Bigl(y_i - \\overline{y}_{R_m}\\Bigr)^2,\n",
        "   $$\n",
        "   where\n",
        "   $$\n",
        "   \\overline{y}_{R_m} = \\frac{1}{\\lvert R_m \\rvert}\n",
        "   \\sum_{i \\in R_m} y_i\n",
        "   $$\n",
        "   is the mean response in leaf $m$.\n",
        "\n",
        "\n",
        "## **Research Questions**\n",
        "\n",
        "1. **Equivalence Analysis**  \n",
        "   Determine whether the above formulations are equivalent or if they can yield different split choices. Specifically:\n",
        "   - Are *local RSS minimization* and *RSS gain maximization* equivalent?\n",
        "   - Does *total RSS minimization* coincide with either of these two, or is it distinct?\n",
        "   \n",
        "2. **Empirical Experiment**  \n",
        "   Design and conduct a Python experiment to determine which of these formulations is implemented in `scikit-learn` in `DecisionTreeRegressor`. Present numerical results and plots to support your conclusion.\n",
        "\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "1. **Formulation Analysis**  \n",
        "   - Compare *local RSS minimization*, *RSS gain maximization*, and *total RSS minimization*.\n",
        "   - If you find that any pair of formulations is equivalent, provide a concise proof.  \n",
        "   - If you find that they differ, construct a counterexample.\n",
        "\n",
        "2. **Empirical Verification**  \n",
        "   - Create a small artificial dataset and train a `DecisionTreeRegressor` from `scikit-learn`.\n",
        "   - The dataset must be designed in a way that uniquely identifies the formulation used. Provide a short code snippet and a plot or table to support your conclusion.\n",
        "\n",
        "3. **Report**  \n",
        "   - Summarize your theoretical insights and empirical findings in a **Colab notebook**.\n",
        "   - Include the relevant proofs, code, discussion, and conclusions.\n",
        "   - Place the notebook in your **GitHub repository** for this course, add a link to it in your README.md and add an **“Open in Colab”** badge in the notebook so it can be launched directly.\n",
        "\n"
      ],
      "metadata": {
        "id": "CiiBWLwyz6PI"
      },
      "id": "CiiBWLwyz6PI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zacznijmy od porównania local RSS minimalization oraz RSS gain maximization.\n",
        "Oczywiście są one równoważne:\n",
        "skoro:\n",
        "$\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2$ nie zależy od wyboru cechy $X_j$, to:\n",
        "$$\n",
        "    \\arg\\max_{X_j, t} \\Bigl\\{\n",
        "   \\underbrace{\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2}_{\\text{Parent RSS}}\n",
        "   \\;-\\;\n",
        "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
        "   \\Bigr\\} =\n",
        "   $$\n",
        "   $$\\arg\\max_{X_j, t} \\Bigl\\{\n",
        "   \\;-\\;\n",
        "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
        "   \\Bigr\\}=$$\n",
        "   $$\n",
        "   = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.\n",
        "   $$\n",
        "   \n",
        "\n",
        "   Podobnie równoważne będzie Total RSS Minimization.\n",
        "   Zobaczmy, że Total RSS będzie sumą RSS \"starych liści\" z wyłączeniem tego z którego powstaną dwa nowe liście (oznaczonego jako P) i dwóch nowych liści (R1 i R2), wtedy:\n",
        "\n",
        "   $$\\text{TotalRSS}(T(X_j, t)) = \\left( \\sum_{m \\in \\mathrm{Leaves}(T), m \\neq P} \\text{RSS}(m) \\right) + \\text{RSS}(R_1(X_j, t)) + \\text{RSS}(R_2(X_j, t))$$\n",
        "   Skoro zatem pierwszy składnik sumy jest ponownie niezależny od wybranej cechy (a jedynie od wybranego liścia), to:\n",
        "   $$\\arg\\min_{X_j, t} \\text{TotalRSS}(T(X_j, t)) = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\text{RSS}(R_m(X_j, t)) = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.$$\n",
        "\n",
        "\n",
        "   Pokazaliśmy równość między wszystkimi trzema, dalsza część zadania jest zatem niemożliwa."
      ],
      "metadata": {
        "id": "DPz0Qi-ImgIL"
      },
      "id": "DPz0Qi-ImgIL"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}